# AI Safety Evaluation Pipeline

Built a Python pipeline to evaluate truth-seeking in LLMs using a 1000-item dataset.
- Dataset: 1000 question-answer pairs (80% truthful, 20% deceptive)
- Model: distilbert-base-uncased-finetuned-sst-2-english
- Metrics: Accuracy, F1 Score
- Run: `sh scripts/run_all.sh`

